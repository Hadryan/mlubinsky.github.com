
<head>
 <link rel="stylesheet" href="style.css">
</head>

<body>
 <pre>
<h2>Big Data Architecture, Kafka</h2>
https://itnext.io/from-monoliths-to-microservices-b6b851ab43e3

https://dzone.com/articles/kylo-self-service-data-ingestion-cleansing-and-val  Apachi NiFi Kylo
 https://habr.com/company/sberbank/blog/353608/ Kafka
 https://habr.com/company/piter/blog/352978/ Kafka
 https://habr.com/post/354486/  Kafka
 https://habr.com/company/skbkontur/blog/353204/ Kafka
 https://www.youtube.com/watch?v=eublKlalobg&feature=youtu.be
 https://speakerdeck.com/vikgamov/devnexus-2018-apache-kafka-a-streaming-data-platform
 https://www.confluent.io/blog/ksql-in-action-enriching-csv-events-with-data-from-rdbms-into-AWS/ KAFKA
 https://medium.com/tecnolog%C3%ADa/how-we-built-a-streaming-analytics-solution-using-apache-kafka-druid-66c257adcd9a 
 https://assets.ctfassets.net/oxjq45e8ilak/1y637HHnSQQMewS0m4usYS/95ffde03d09c3f49dcc7fe85fc976553/Gamov_Kafka_EOS.pdf
 https://habr.com/company/jugru/blog/354238/
 
 https://medium.com/tag/streaming-analytics/archive
 
 <h2>AWS</h2>
 https://servers.lol
 https://serverless.com/
 https://habr.com/company/funcorp/blog/354394/ AWS
 https://read.acloud.guru/six-months-of-serverless-lessons-learned-f6da86a73526 
 https://www.jefclaes.be/2017/12/passing-aws-certified-solutions.html  AWS exam
 https://github.com/JefClaes/amazon-redshift-fundamentals Redshift
 https://www.rainerhahnekamp.com/en/single-instance-ecs-setup/
 https://itnext.io/creating-a-blueprint-for-microservices-and-event-sourcing-on-aws-291d4d5a5817
 https://www.nodexplained.com/blog-detail/2018/04/27/explore-the-amazon-web-services-management-console
 https://medium.com/epsagon/lambda-internals-exploring-aws-lambda-462f05f74076   AWS
 https://itnext.io/creating-a-blueprint-for-microservices-and-event-sourcing-on-aws-291d4d5a5817 
 https://blog.sicara.com/deploy-serverless-lambda-s3-api-aws-2cf99b8f34ae
 https://itnext.io/create-ftp-on-aws-ec2-f6e57d4d7f25   FTP on AWS
 https://read.acloud.guru/how-we-built-a-big-data-analytics-platform-on-aws-for-100-large-users-for-under-2-a-month-b37425b6cc4
 https://www.javacodegeeks.com/tag/amazon-aws
 
 https://habr.com/post/353734/ Distributed
 
 http://tech.marksblogg.com/presto-connectors-kafka-mongodb-mysql-postgresql-redis.html Presto
<h2>Hadoop</h2>
 
  conf/core-site.xml:  
  conf/hdfs-site.xml:                      
  conf/mapred-site.xml:                            

  
 mapreduce.input.fileinputformat.split.maxsize
 mapreduce.input.fileinputformat.split.minsize

 <a href=hadoop.txt>My Notes</a>
 
<a href=http://www.idryman.org/blog/2014/03/05/hadoop-performance-tuning-best-practices>Performance tuning</a>
 
<a href=https://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance>Performance</a>

<a href=https://mapr.com/blog/database-comparison-an-in-depth-look-at-mapr-db>MapR</a>

<h2>Consistent Hashing</h2>
<a href=https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8>Consistent hashing</a>
https://dzone.com/articles/simple-magic-consistent

https://orc.apache.org/
https://www.java-success.com/01-hadoop-bigdata-overview-interview-questions-answers/

<a href=https://www.amazon.com/dp/1946383481>Hadoop BIG DATA Interview Questions</a>
<a href=https://www.amazon.com/dp/1946383228>Java notes</a>

<h2>Avro </h2>
Like Thrift, protobuf structures are defined via an IDL, which is used to generate stub code for multiple languages.
Also like Thrift, Protocol Buffers do not support internal compression of records, are not splittable, 
and have no native MapReduce support. But also like Thrift, the Elephant Bird project can be used to encode protobuf records, 
providing support for MapReduce, compression, and splittability.

Avro is a language-neutral data serialization system designed to address the major downside of Hadoop Writables:
lack of language portability. Like Thrift and Protocol Buffers, Avro data is described through a language-independent schema. 
Unlike Thrift and Protocol Buffers, code generation is optional with Avro. Since Avro stores the schema in the header of each file,
it’s self-describing and Avro files can easily be read later, even from a different language than the one used to write the file. 
Avro also provides better native support for MapReduce since Avro data files are compressible and splittable. 
Another important feature of Avro that makes it superior to SequenceFiles for Hadoop applications is support for schema evolution;
that is, the schema used to read a file does not need to match the schema used to write the file.
This makes it possible to add new fields to a schema as requirements change.
Avro schemas are usually written in JSON, but may also be written in Avro IDL, which is a C-like language. As just noted,
the schema is stored as part of the file metadata in the file header. In addition to metadata, 
the file header contains a unique sync marker. Just as with SequenceFiles, this sync marker is used to separate blocks in the file,
allowing Avro files to be splittable. Following the header, an Avro file contains a series of blocks containing serialized Avro objects.
These blocks can optionally be compressed, and within those blocks, types are stored in their native format, providing an additional
boost to compression. At the time of writing, Avro supports Snappy and Deflate compression.

<h2>Columnar file formats for Hadoop </h2>
Columnar file formats supported on Hadoop include the RCFile format, which has been popular for some time as a Hive format,
as well as newer formats such as the Optimized Row Columnar (ORC) and Parquet,

The MapReduce framework and other ecosystem projects provide RecordReader implementations for many file formats:
text delimited, SequenceFile, Avro, Parquet, and more. 


<h2>HBASE</h2>
HBase records can have an unlimited number of columns, but only a single row key. 
This is different from relational databases, where the primary key can be a composite of multiple columns. 
This means that in order to create a unique row key for records, you may need to combine multiple pieces of information in a single key.

In HBase, all the row keys are sorted, and each region stores a range of these sorted row keys. 
Each region is pinned to a region server (i.e., a node in the cluster). 
A well-known anti-pattern is to use a timestamp for row keys because it would make most of the put and get requests 
  focused on a single region and hence a single region server, which somewhat defeats the purpose of having a distributed system.
It’s usually best to choose row keys so the load on the cluster is fairly distributed;
  one of the ways to resolve this problem is to salt the keys.
  
 A table can have one or more column families.   
Each column family has its own set of HFiles and gets compacted independently of other column families in the same table.

a) Get is used to read information from the HBase table. 
You can read a specific column from a column-family in a table. 
b) Put writes the information to the HBase table. 
You can write specifically to a column in a column-family in a table. 
For an insert, you need to just specify the table name and column data to fill.
For an update, you need to Scan the row first using the column and cell values and then update the new value along with the timestamp. 
c) For Delete, mention the table name and version / column / column family to delete. 
d) Scan is used access the entire table or a set of records. 
e) Increment is used to automatically increment a cell value (row or column). 

 How do you read and write data using HBase?
 Answer: Data is Written into HBase using the following steps:
 WAL stores the information to be written for log purposes. 
 This data is copied into the MemStore which is a temporary memory like RAM.
 The MemStore makes HBase faster. From the MemStore, the data is dumped into the HFile which is in the HDFS. 
 If the MemStore cache is full, the data is directly written into the HFile.
 Once data is written successfully into the HDFS, an acknowledgement is passed on to the client.

<h2>Hive</h2>
Hive metastore. To enable the usage of Hive metastore outside of Hive, a separate project called HCatalog was started. 
  HCatalog is a part of Hive and serves the very important purpose of allowing other tools (like Pig and MapReduce)
  to integrate with the Hive metastore.

 Physically, a partition in Hive is nothing but just a sub-directory in the table directory.
CREATE TABLE table_name (column1 data_type, column2 data_type) 
PARTITIONED BY (partition1 data_type, partition2 data_type,….);
 
Partitioning is works better when the cardinality of the partitioning field is not too high .

https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive
http://www.hadooptpoint.org/difference-between-partitioning-and-bucketing-in-hive/

Clustering aka bucketing on the other hand, will result with a fixed number of files, 
since you do specify the number of buckets. 
What Hive will do is to take the field, calculate a hash and assign a record to that bucket.
But what happens if you use let's say 256 buckets and the field you're bucketing on has a low cardinality 
(for instance, it's a US state, so can be only 50 different values) ? 
You'll have 50 buckets with data, and 206 buckets with no data.

CREATE TABLE table_name PARTITIONED BY (partition1 data_type, partition2 data_type,….) 
CLUSTERED BY (column_name1, column_name2, …) 
SORTED BY (column_name [ASC|DESC], …)] 
INTO num_buckets BUCKETS;

Partitions can dramatically cut the amount of data you're querying.
 if you want to query only from a certain date forward, the partitioning by year/month/day is going to dramatically cut the amount of IO.
bucketing can speed up joins with other tables that have exactly the same bucketing, 
 if you're joining two tables on the same employee_id, hive can do the join bucket by bucket
 (even better if they're already sorted by employee_id since it's going to to a mergesort which works in linear time).

So, bucketing works well when the field has high cardinality and data is evenly distributed among buckets. 
Partitioning works best when the cardinality of the partitioning field is not too high.

Also, you can partition on multiple fields, with an order (year/month/day is a good example),
while you can bucket on only one field.

<h2>HDFS</h2>
- rename / move file or directory
hdfs dfs -mv /old/file /new/file
- get admin report / cluster status
hdfs dfsadmin -report
- get rack awareness and number of under replicated blocks
hadoop fsck / -locations -blocks -files | grep -i -C6 miss
- print a compressed file uncompress a file
hdfs dfs -text /path/to/compressed_file
- merge all files in a folder to one file
hdfs dfs -getmerge /path/to/folder /path/to/file
- rm remove file
hdfs dfs -rm /user/hive/warehouse/blah/blah
- kill yarn job
hadoop job -kill job_id
- empty trash
hdfs dfs -expunge
- stream result of pipe stdin to a hadoop file
cat somefile.tsv | hdfs dfs -put - /file/on/cluster.tsv
- put copy file from local to cluster
hdfs dfs -put localfile /user/hadoop/hadoopfile
- create touch a file
hdfs dfs -touchz /path/to/myfile.txt
- tail a file
hdfs dfs -tail /my_file
- return stat information of the path (basic file info)
hdfs dfs -stat /my_file
- delete folder
hdfs dfs -rmr /folder/to/delete
- delete file
hdfs dfs -rm /file/to/delete
- list all files in a directory
hdfs dfs -ls /user/dir1
- get access control list ACL of files and directories
hdfs dfs -getfacl /file
- display size of files and directories
hdfs dfs -du /user/hadoop/dir1
- copy file within the cluster
hdfs dfs -cp /source_file /dest_file
- count number of directories, files and bytes
hdfs dfs -count /hdfs/folder/file
- copy from local machine to cluster with overwrite
hdfs dfs -copyFromLocal -f /local/file.csv /hdfs/folder/
- copy from remote cluster to local machine
hdfs dfs -copyToLocal /hdfs/folder/file.csv /local/folder
- copy from local machine to cluster
hdfs dfs -copyFromLocal /local/file.csv /hdfs/folder/
- stream append to a file using unix pipe stdin
cat somefile | hdfs dfs -appendToFile - /hdfs/my_file.txt
- append to a file
hdfs dfs -appendToFile /local_file.txt /hdfs/my_file.txt
- print a file
hdfs dfs -cat /my_file
-  change group of file
hdfs dfs -chgrp group_name /my_file
- change file permissions
hdfs dfs -chmod 1755 /my_file
- change owner of file/directory
hdfs dfs -chown user:group /my_hdfs/file
- set replication factor recursive
hdfs dfs -setrep -R -w 1 /my_hdfs/folder
- change replication factor of a file
hdfs dfs -setrep -w 2 /my_hdfs/file
- Copy Files from one cluster to other
hadoop distcp -pb -overwrite hftp://hdfssource:50070/file/to/copy hdfs://hdfsdest:8020/user/test
- print sequence file
hdfs dfs -text /path/to/file/hdfs
- create directory
hdfs dfs -mkdir /path/to/directory

- hadoop streaming
hadoop jar $HADOOP_HOME/hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /bin/wc \
    -file myPythonScript.py

- set sticky bit (preventing anyone except the superuser, owner from deleting or moving the files within the directory)
sudo -u hdfs hadoop fs -chmod 1777 /tmp

</pre>
