
<head>
 <link rel="stylesheet" href="style.css">
</head>

<body>
<!--
<pre>
-->
<h2>Hadoop</h2>
<pre> 
 <a href=hadoop.txt>My Notes</a>
 
<a href=http://www.idryman.org/blog/2014/03/05/hadoop-performance-tuning-best-practices>Performance tuning</a>
 
<a href=https://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance>Performance</a>

<a href=https://mapr.com/blog/database-comparison-an-in-depth-look-at-mapr-db>MapR</a>

<a href=https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8>Consistent hashing</a>

https://orc.apache.org/
https://www.java-success.com/01-hadoop-bigdata-overview-interview-questions-answers/

<a href=https://www.amazon.com/dp/1946383481>Hadoop BIG DATA Interview Questions</a>
<a href=https://www.amazon.com/dp/1946383228>Java notes</a>

Like Thrift, protobuf structures are defined via an IDL, which is used to generate stub code for multiple languages.
Also like Thrift, Protocol Buffers do not support internal compression of records, are not splittable, 
and have no native MapReduce support. But also like Thrift, the Elephant Bird project can be used to encode protobuf records, 
providing support for MapReduce, compression, and splittability.

Avro is a language-neutral data serialization system designed to address the major downside of Hadoop Writables:
lack of language portability. Like Thrift and Protocol Buffers, Avro data is described through a language-independent schema. 
Unlike Thrift and Protocol Buffers, code generation is optional with Avro. Since Avro stores the schema in the header of each file,
it’s self-describing and Avro files can easily be read later, even from a different language than the one used to write the file. 
Avro also provides better native support for MapReduce since Avro data files are compressible and splittable. 
Another important feature of Avro that makes it superior to SequenceFiles for Hadoop applications is support for schema evolution;
that is, the schema used to read a file does not need to match the schema used to write the file.
This makes it possible to add new fields to a schema as requirements change.
Avro schemas are usually written in JSON, but may also be written in Avro IDL, which is a C-like language. As just noted,
the schema is stored as part of the file metadata in the file header. In addition to metadata, 
the file header contains a unique sync marker. Just as with SequenceFiles, this sync marker is used to separate blocks in the file,
allowing Avro files to be splittable. Following the header, an Avro file contains a series of blocks containing serialized Avro objects.
These blocks can optionally be compressed, and within those blocks, types are stored in their native format, providing an additional
boost to compression. At the time of writing, Avro supports Snappy and Deflate compression.

<h2>Columnar file formats   for Hadoop </h2>
Columnar file formats supported on Hadoop include the RCFile format, which has been popular for some time as a Hive format,
as well as newer formats such as the Optimized Row Columnar (ORC) and Parquet,

<h2>HBASE</n2>
HBase records can have an unlimited number of columns, but only a single row key. 
This is different from relational databases, where the primary key can be a composite of multiple columns. 
This means that in order to create a unique row key for records, you may need to combine multiple pieces of information in a single key.

In HBase, all the row keys are sorted, and each region stores a range of these sorted row keys. 
Each region is pinned to a region server (i.e., a node in the cluster). 
A well-known anti-pattern is to use a timestamp for row keys because it would make most of the put and get requests 
  focused on a single region and hence a single region server, which somewhat defeats the purpose of having a distributed system.
It’s usually best to choose row keys so the load on the cluster is fairly distributed;
  one of the ways to resolve this problem is to salt the keys.
  
  
 mapreduce.input.fileinputformat.split.maxsize
 mapreduce.input.fileinputformat.split.minsize

<h2>HDFS</h2>


- rename / move file or directory
hdfs dfs -mv /old/file /new/file
- get admin report / cluster status
hdfs dfsadmin -report
- get rack awareness and number of under replicated blocks
hadoop fsck / -locations -blocks -files | grep -i -C6 miss
- print a compressed file uncompress a file
hdfs dfs -text /path/to/compressed_file
- merge all files in a folder to one file
hdfs dfs -getmerge /path/to/folder /path/to/file
- rm remove file
hdfs dfs -rm /user/hive/warehouse/blah/blah
- kill yarn job
hadoop job -kill job_id
- empty trash
hdfs dfs -expunge
- stream result of pipe stdin to a hadoop file
cat somefile.tsv | hdfs dfs -put - /file/on/cluster.tsv
- put copy file from local to cluster
hdfs dfs -put localfile /user/hadoop/hadoopfile
- create touch a file
hdfs dfs -touchz /path/to/myfile.txt
- tail a file
hdfs dfs -tail /my_file
- return stat information of the path (basic file info)
hdfs dfs -stat /my_file
- delete folder
hdfs dfs -rmr /folder/to/delete
- delete file
hdfs dfs -rm /file/to/delete
- list all files in a directory
hdfs dfs -ls /user/dir1
- get access control list ACL of files and directories
hdfs dfs -getfacl /file
- display size of files and directories
hdfs dfs -du /user/hadoop/dir1
- copy file within the cluster
hdfs dfs -cp /source_file /dest_file
- count number of directories, files and bytes
hdfs dfs -count /hdfs/folder/file
- copy from local machine to cluster with overwrite
hdfs dfs -copyFromLocal -f /local/file.csv /hdfs/folder/
- copy from remote cluster to local machine
hdfs dfs -copyToLocal /hdfs/folder/file.csv /local/folder
- copy from local machine to cluster
hdfs dfs -copyFromLocal /local/file.csv /hdfs/folder/
- stream append to a file using unix pipe stdin
cat somefile | hdfs dfs -appendToFile - /hdfs/my_file.txt
- append to a file
hdfs dfs -appendToFile /local_file.txt /hdfs/my_file.txt
- print a file
hdfs dfs -cat /my_file
-  change group of file
hdfs dfs -chgrp group_name /my_file
- change file permissions
hdfs dfs -chmod 1755 /my_file
- change owner of file/directory
hdfs dfs -chown user:group /my_hdfs/file
- set replication factor recursive
hdfs dfs -setrep -R -w 1 /my_hdfs/folder
- change replication factor of a file
hdfs dfs -setrep -w 2 /my_hdfs/file
- Copy Files from one cluster to other
hadoop distcp -pb -overwrite hftp://hdfssource:50070/file/to/copy hdfs://hdfsdest:8020/user/test
- print sequence file
hdfs dfs -text /path/to/file/hdfs
- create directory
hdfs dfs -mkdir /path/to/directory

- hadoop streaming
hadoop jar $HADOOP_HOME/hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper myPythonScript.py \
    -reducer /bin/wc \
    -file myPythonScript.py

- set sticky bit (preventing anyone except the superuser, owner from deleting or moving the files within the directory)
sudo -u hdfs hadoop fs -chmod 1777 /tmp

</pre>
