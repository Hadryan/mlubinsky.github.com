<head>
 <link rel="stylesheet" href="style.css">
</head>

<body>
<!--
<pre>
-->
<h2>Spark</h2>

<pre>
 A dataframe in Spark is similar to a SQL table, an R dataframe, or a pandas dataframe. 
 In Spark, dataframe is actually a wrapper around RDDs, the basic data structure in Spark. 

https://www.slideshare.net/cloudera/top-5-mistakes-to-avoid-when-writing-apache-spark-applications

<h2>Read</h2>

val dataLakeDF = spark.read.parquet("s3a://some-bucket/foo")

val extractDF = dataLakeDF
  .where(col("mood") === "happy")
  .repartition(10000)

spark.read.format(" csv")
.option(" mode", "FAILFAST")   //dropMalformed    | permissive
.option(" inferSchema", "true") 
.option(" path", "path/ to/ file( s)") 
.schema( someSchema) 
.load()

spark.read.format(" parquet") 
.load("/ data/ flight-data/ parquet/ 2010-summary.parquet")
.show( 5) 

<h2>Write</h2>

 DataFrameWriter
 .format(...)
 .option(...)
 .partitionBy(...)
 .bucketBy(...)
 .sortBy( ...)
 .save()

 
 dataframe.write.format(" csv") 
 .option(" mode", "OVERWRITE") 
 .option(" dateFormat", "yyyy-MM-dd") 
 .option(" path", "path/ to/ file( s)") 
 .save()

<h2>Query</h2>

dbDataFrame.select(" DEST_COUNTRY_NAME"). distinct(). show( 5)
dbDataFrame.select(" DEST_COUNTRY_NAME"). distinct(). explain
= = Physical Plan = =
*HashAggregate( keys =[ DEST_COUNTRY_NAME# 8108], functions =[]) 
 +- Exchange hashpartitioning( DEST_COUNTRY_NAME# 8108, 200) 
  +- *HashAggregate( keys =[ DEST_COUNTRY_NAME# 8108], functions =[]) 
    +- *Scan JDBCRelation( flight_info) [numPartitions = 1] ... 
    
Spark can actually do better than this on certain queries. 
For example, if we specify a filter on our DataFrame, Spark will push that filter down into the database.
We can see this in the explain plan under PushedFilters.  
dbDataFrame.filter(" DEST_COUNTRY_NAME in (' Anguilla', 'Sweden')"). explain # 
 = = Physical Plan = = 
 *Scan JDBCRel... 
   PushedFilters: [* In( DEST_COUNTRY_NAME, [Anguilla, Sweden])],

val pushdownQuery = """( SELECT DISTINCT( DEST_COUNTRY_NAME) FROM flight_info) AS flight_info"""

val dbDataFrame = spark.read.format(" jdbc") 
.option(" url", url)
.option(" dbtable", pushdownQuery)
.option(" driver", driver) 
.load()
 
 
val joinExpr = person.col(" graduate_program") = = = graduateProgram.col(" id") 
person
 .join( graduateProgram, joinExpr)
. explain()
  
 = = Physical Plan = = *BroadcastHashJoin [graduate_program# 40], 
 [id# 5.... 
 :- LocalTableScan [id# 38, name# 39, graduate_progr... 
 +- BroadcastExchange HashedRelationBroadcastMode(.... 
 +- LocalTableScan [id# 56, degree# 57, departmen.... 
 
  
 With the DataFrame API, we can also explicitly give the optimizer a hint
 that we would like to use a broadcast join by using the correct function around the small DataFrame in question.
 In this example, these result in the same plan we just saw; however, this is not always the case: 
 
 import org.apache.spark.sql.functions.broadcast 
 val joinExpr = person.col(" graduate_program") = = = graduateProgram.col(" id") 
 person.join( broadcast( graduateProgram), joinExpr). explain() 
 
 The SQL interface also includes the ability to provide hints to perform joins. 
 These are not enforced, however, so the optimizer might choose to ignore them. 
 You can set one of these hints by using a special comment syntax. 
 MAPJOIN, BROADCAST, and BROADCASTJOIN all do the same thing and are all supported: 
 -- in SQL 
 SELECT /* + MAPJOIN( graduateProgram) */ * FROM person JOIN graduateProgram ON person.graduate_program = graduateProgram.id
 
 This doesn’t come for free either: if you try to broadcast something too large, you can crash your driver node
 (because that collect is expensive).

<h2>Monitoring</h2>
  To change Spark’s log level, simply run the following command: 
  spark.sparkContext.setLogLevel(" INFO")

<h2>Links</h2>
  
https://medium.com/@mrpowers/how-to-write-spark-etl-processes-df01b0c1bec9
https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4
https://medium.com/@mrpowers/chaining-custom-dataframe-transformations-in-spark-a39e315f903c

http://spark.apache.org/examples.html
https://mindfulmachines.io/blog/2018/4/3/spark-rdds-and-dataframes
https://github.com/mahmoudparsian/pyspark-tutorial
https://medium.com/@schaun.wheeler/pyspark-udfs-and-star-expansion-b50f501dcb7b
http://www.janvsmachine.net/2017/09/sessionization-with-spark.html
https://spark.apache.org/docs/2.2.0/
https://spark.apache.org/docs/0.7.2/api/pyspark/pyspark.rdd.RDD-class.html

https://blog.knoldus.com/2017/04/16/the-dominant-apis-of-spark-datasets-dataframes-and-rdds/
http://parrotprediction.com/partitioning-in-apache-spark/
https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/
https://opencredo.com/spark-testing/
https://www.youtube.com/watch?v=ISO8V1cYmfY
https://www.slideshare.net/hadooparchbook/top-5-mistakes-when-writing-spark-applications-

https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/
https://blog.deepsense.ai/improve-aggregate-performance-with-batching/66374492
http://technology.finra.org/code/using-spark-transformations-for-mpreduce-jobs.html
https://blog.deepsense.ai/fast-and-accurate-categorical-distribution-without-reshuffling-in-apache-spark/


<h3>Start</h3>
In Spark all work is expressed as either
 - creating new RDDs
 - transforming existing RDDs
 - calling operations on RDDs to compute a result.

DataSet: static typing -> compile-time error detection
DataFrames are untyped: scala compiler doesn not check types in its schema
DataFrames=DataSet[Rows]


Spark supports two types of shared variables:
    1)  broadcast variables - can be used to cache a value in memory on all nodes, 
    2) accumulators -  are only “added” to, such as counters and sums.

There are two types of transformations:
 - those that specify narrow dependencies
 - those that specify wide dependencies

Transformations consisting of narrow dependencies are those where each input partition will contribute to only one output partition.
Example: where clause specifies a
narrow dependency, where only one partition contributes to at most one output partition.

A wide dependency style transformation will have input partitions contributing to many output partitions. 
We call this a shuffle where Spark will exchange partitions across the cluster. 
Spark will automatically perform an operation called pipelining on narrow dependencies, 
this means that if we specify multiple filters on DataFrames they’ll all be performed in memory.
The same cannot be said for shuffles. When we perform a shuffle, Spark will write the results to disk.

There are three kinds of actions:
• actions to view data in the console;
• actions to collect data to native objects in the respective language; 
• and actions to write to output data sources.


Spark SQL allows you as a user to register any DataFrame as a table or view (a temporary table) and query it using pure SQL. 
There is no performance difference between writing SQL queries or writing DataFrame code, 
they both “compile” to the same underlying plan that we specify in DataFrame code.

<h2>Structured Streaming</h2>
Structured Streaming became Production Ready in Spark 2.2. 

Structured streaming allows you to take the same operations that you perform in batch mode and perform them in a streaming fashion. 
This can reduce latency and allow for incremental processing. 
The best thing about Structured Streaming is that it allows you to rapidly and quickly get value out of streaming systems with simple switches, it also makes it easy to reason about because you can write your batch job as a way to prototype it and then you can convert it to streaming job. The way all of this works is by incrementally processing that data.

<h3>Spark Session </h3>
val spark=SparkSession
  .builder()
  .appName("My App")
  .getOrCreate()

val conf = new SparkConf().setAppName("SparkJoins").setMaster("local")
val sc = new SparkContext(conf)

Broadcast variables allow the programmer to keep a read-only variable or lookup table cached on each machine
SparkContext.broadcast(v)
val broadcastVar = sc.broadcast(Array(1, 2, 3))

def processData (t: RDD[(Int, Int)], u: RDD[(Int, String)]) : Map[Int,Long] = {
  var jn = t.leftOuterJoin(u).values.distinct
  return jn.countByKey
}

Accumulators are variables that are only “added” to through an associative operation and can therefore, be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums.
Spark natively supports accumulators of numeric types, and programmers can add support for new types

val accum = sc.accumulator(0)
sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
accum.value


 StatusCounter scounter = myRDD.status();
 scounter.count() // Mean() Sum Max Min  Variance StdDev

<h3>RDD - distributed and read-only </h3>
 partitions - atomic pieces  of data set
 dependencies  - relation between RDD and how it was derived from
 function for computing datset based on parent RDD
 metadata

you will likely only be creating two types of RDDs: 
the “generic” RDD type 
or 
a key-value RDD that provides additional functions, such as aggregating by key. 

Both just represent a collection of objects, 
but key-value RDDs have special operations as well as a concept of custom partitioning by key. 
Let’s formally define RDDs. 
Internally, each RDD is characterized by five main properties:
1. A list of partitions 
2. A function for computing each split 
3. A list of dependencies on other RDDs
4. Optionally, a Partitioner for key-value RDDs (e.g., to say that the RDD is hash-partitioned) 
5. Optionally, a list of preferred locations on which to compute each split (e.g., block locations for a Hadoop Distributed File System [HDFS] file) 


The Partitioner is probably one of the core reasons why you might want to use RDDs in your code. 
Specifying your own custom Partitioner can give you significant performance and stability improvements if you use it correctly.

You manipulate RDDs in much the same way that you manipulate DataFrames. 
the core difference being that you manipulate raw Java or Scala objects instead of Spark types.

Narrow depencencies: each partition of parent RDD is used by at most one partition of child RDD
fast, no shuffle, optimizations like pipelining possible
    Parent partition(1)  -> Child partition(<=1)
Examples: map, filter, flatMap, mapPartitions, mapValues, mapPartitionsWithIndex, union, join with co-partitioned inputs

Wide depencencies: slow... shuffle over network
    Parent partition(1)  -> Child partition(many)
Examples:
(group/reduce/combine)ByKey, join, cogroup groupWith, join (depends on partitioning scheme), (left/right)OuterJoin
distinct, intersection, repartition, coalese

dependencies method on RDDs returns the sequence of Dependencies objects
The sorts of dependencies objects:
Narrow dependencies objects:  OneToOneDependency PruneDependency, RangeDependency
Wide dependency objects: ShuffleDependency

val wordsRdd = sc.parallelize(largeList)
val pairs = wordRdd.map(c=>(c,1))
 .groupByKey()
 .dependencies


<h3>SparkWordCount</h3>

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark._

object SparkWordCount {
   def main(args: Array[String]) {

      val sc = new SparkContext( "local", "Word Count", "/usr/local/spark", Nil, Map(), Map())

      /* local = master URL; Word Count = application name; */
      /* /usr/local/spark = Spark Home; Nil = jars; Map = environment */
      /* Map = variables to work nodes */
      /*creating an inputRDD to read text file (in.txt) through Spark context*/
      val input = sc.textFile("in.txt")
      /* Transform the inputRDD into countRDD */

      val count = input.flatMap(line ⇒ line.split(" "))
      .map(word ⇒ (word, 1))
      .reduceByKey(_ + _)

      /* saveAsTextFile method is an action that effects on the RDD */
      count.saveAsTextFile("outfile")
      System.out.println("OK");
   }
}

scalac -classpath "spark-core_2.10-1.3.0.jar:/usr/local/spark/lib/spark-assembly-1.4.0-hadoop2.6.0.jar" SparkPWordCount.scala
jar -cvf wordcount.jar SparkWordCount*.class spark-core_2.10-1.3.0.jar/usr/local/spark/lib/spark-assembly-1.4.0-hadoop2.6.0.jar
spark-submit --class SparkWordCount --master local wordcount.jar

<h3> Choosing best excecution plan</h3>
case class Demographic (
 id: Int
 codingBootcamp: Boolean,
 country: String
 gender: String
 isMinority: Boolean,
 servedInMilitary: Boolean
 )
//Pair RDD (id,demographic)
 val demographic = sc.textfile(..)

 case class Finances (id: Int
                      hasDept: Boolean,
                      hasDependents: Boolean,
                      hasLoans: Boolean,
                      income: Int
 )
//Pair RDD (id,finances)
val finances = sc.textfile(..)
<b>plan #1</b>
Following join  is pairRDD (id:Int, (Demographic, Finances))
demographic.join(finances)
   .filter { p=>
             p._2._1.country == "Swiss" &&
             p._2._2..hasDependens &&
             p._2._2.hasDept
}.count

steps for join above are:
1) inner join
2) filter to select Swiss people
3) filter to select dept and dependence

<b>plan #2 : fast filter before join </b>
val filtered_finance=finances.filter(p=>p._2.hasDependends && p._2.hasDept)
demographic.filter(p=>p._2.country == "Swiss")
   .join(filtered_finance)
   count
steps for join above are:
1) filter to select dept and dependence
2) filter to select Swiss people
3) join

<b>plan #3 slowest</b>
val cartesian=demographic.catresian(finances)
cartesian.filter{
   case (p1,p2) => p1._1 == p2._1
}
.filter{
	case (p1,p2) => (p1._2.country == "Swiss") &&
                    (p2._2.hasDependence) &&
                    p2._2.hasDept
}.count



<h3>Spark SQL </h3>
It makes optimization of excecution plan as SQL RDBMS does
DataFrames are untyped: scala compiler doesn not check types in its schema
RDD[T]  but DataFrame has no type, it con
 DataSets
Catalyst - query optimizer
Tungsten - off-heap serializer avoiding garbage collector - contains rows which can contain any schema
DataFrame  conseptually are RDD full of records with known schema (like in SQL RDBMS table)
Transformations  on DataFrames are known as untyped transformations
DataFrame can be created from existing RDD
  val tupleRDD =  ..//Assume RDD[(Int String, String, String)]
  val tupleDF = tupleRDD.toDF("id","name","city","country")
if you have RDD containing case class instances then spark can infere attributes from case class fields


case class Person(id: Int, name: String, city: String)
val peopleRDD = .. //Assume RDD[Person]
reflection will be used to automatically generate names of fields


1) Create RDD of Rows from original RDD
2) Create schema represented by StructuredType
3) Apply schema to RDD of Rows via createDataFrime method

or by reading datasource from json/csv/Parquet/ JDBC file:
val df=sparkread.json(my.json)

peopleOF.createOrReplaceTempView("people")
val adultsDF = spark.sql("SELECT * FROM people WHERE age>17")

Complex Spark SQL Data Types
Scala     ->  SQL
Array[T]  -> ArrayType(elementType, containsNull)
Map[K,V]  -> MapType(keyType, valueType, valueContainsNull)
case class ->  StructType(List[StructFields])

DataFrame show() show 20 first elements
printSchema()


df.filter($"age" >18)
df.filter(df("age") >18)
df.filter("age >18")
val res=empDF.select("id","lname")
              .where("city=='Sidney'")
              .orderBy("id")


myDF.groupBy($authorId, $subForum)
  .agg(c, count($authorId))
  .orderBy($subForum, count($authorId))
  .desc

<h3>Pair RDD </h3>
 def groupByKey(): RDD[(K,Iterable[V])]
 def reduceByKey(func: (V,V) => V): RDD[(K,V)]
 def join[W](other: RDD[(K,W)]): RDD[(K,(V,W))]

 var pairRDD = rdd.map(page => (title, page.text))

 case class Event(organizer: String, name: String, budget: Int)   //key:organizer
 var eventsRDD = sc.parallelize(...)
     .map(event => (event.orgazier, event.budget))

 def groupByKey(): RDD[(K, Iterable[V])]

 var groupedRdd= eventsRDD.groupByKey()
 groupedRdd.collect().foreach(println)

 groupByKey collects all of the values associated with the given key and stores them in that single collection. That means that data has moved around the network and doing this, moving the data on the network is called shuffling.  Can we somehow do it in a more efficient way?

Perhaps we can reduce before doing the shuffle. This could potentially reduce the amount of data that we actually have to send over the network.

To do that, we can use the reduceByKey operator, you can think of it as a combination of first doing groupByKey and then reducing overall the values that were grouped by that key in those collections.

reduceByKey: conceptionally it is combination of groupByKey and reducing all values per key
def reduceByKey (func: (V,V) => V): RDD[(K,V)]
val budgetsRDD=eventsRDD.reduceByKey(_+_)
budgetsRdd.collect().foreach(println)

 def mapValues[U] (f: V=>U): RDD[(K,U)]
 rdd.map { case(x,y): (x,func(y))}  //applies func to only values in PairRDD

 def countByKey(): Map[K,Long]) // counts # of elements per key

 keys (def keys: RDD{K}) Return RDD with keys on eack tuple
 Example: number of unique visitors
 case class Visitor(ip: String, timestamp: String, duration: String)
 val visits: RDD[Visitor]= sc.textFile(..).map(v=>(v.ip, v.duration))
 val numUniqueVisits=visits.keys.distinct().count()

 In regular Scala collections there is groupBy:
 def groupBy[K](f: A=>K): Map[K, Traversable[A]]
 Usage example:
 val ages=List(34,12,14,78)
 val grouped=ages.groupBy(
    age =>
    if (age >17) "adult"
    else if (age <50) "adult"
    else "senior"
 )
<h3>Joins on pair RDDs</h3>
Inner/leftOuterJoin/rightOuterJoin

<h3>How to configure RDD persistance</h3>
 - in memory as Java objects
 - on disk as Java objects
 - in memory as serialized Java objects
 - on disk as serialized Java objects
 - both in memory an on disk
 cache()
 persist
 MEMORY_ONLY
 MEMORY_ONLY_SER
 MEMORY_AND_DISK
 MEMORY_AND_DISK_SER
 DISK_ONLY

<h3> Partitioning</h3>
https://www.edureka.co/blog/demystifying-partitioning-in-spark
partition# = key.hashcode() % number_of_partitions
can lead to skewed distribution between nodes

Spark support hash partitioning and range partitioning
all data in same partition are garanteed to be in the same node


Partitions are configurable provided the RDD is key-value based.
Properties of Partition
Tuples in the same partition are guaranteed to be in the same machine.
Each node in a cluster can contain more than one partition.
The total number of partitions are configurable, by default it is set to the total number of cores on all the executor nodes.
Types of Partitioning in Spark
Spark supports two types of partitioning,

- Hash Partitioning: Uses Java’s Object.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions.
- Range Partitioning: Uses a range to distribute to the respective partitions the keys that fall within a range. This method is suitable where there’s a natural ordering in the keys and the keys are non negative.


Pair RDD may contain keys that have order defined (e.g.: Int or String)
For such RDD the range partitioning is more effective

range partitioner : provide # of patitions
provide a pair RDD with <b>ordered keys</b>
this RDD is sampled to create a suitable set of sorted ranges

val pairs = purchasedRdd.map(p=> (p.customerId, p.price))
val tunedPartitioner = new RangePartitioner(8, pairs)   // spark will use sampling here against pairs to find best ranges
val partitioned = pair.partitionBy(tunePartitioner).persist()  // persist to eliminate shuffling
val purchasesPerCustomer=partitioned.map(p=> (p._1), (1.p._2)))
val purchasesPerMonth = purchasesPerCustomer.reduceByKey((v1,v2)=> (v1._1+v2._1,v1._2+v2._2)

Operations on Pair RDD that hold and propagate a partitioner:
cogroup
groupWith
join
leftOterJoin
rightOuterJoin
groupByKey - uses hash partitioner with default parametes
reduceByKey
foldByKey
combineByKey
partitionBy
sort
mapValues (if parent has a partitioner)
flatMapValues (as above)
filter (as above)

But following transformation will loose partitioner because it is possible for map to change the key
map  rdd.map((k: String, v:Int) => ("doh",v))
flatMap


userData (UserId, UserInfo) //big table
events(UserId,LinkInfo)

val sc=new SparkContent()
val userData=sc.sequenceFile[UserId,UserInfo]("hdfs://..").persist()

def processingNewLogs(logfilename: String){
	val events = sc.sequenceFile[UserId,LinkInfo](logfilename)
	val joined = userData.join(events) //RDD (UserId, (UserInfo,LinkInfo))
	val offTopicVisits = join.filter {
	   case (userId, (userInfo, linkInfo)) => //expand tuple
	     ! userInfo.topics.contains(linkInfo.topic)
	}.count()
	println("Number of visits to not-subscribed topics: "+offTopicVisits)
}

The code above is slow because of shuffling; fix is - partition
val userData=sc.sequenceFile[UserId,UserInfo]("hdfs://..")
  .partitionBy(new HashPartitioner(100))
  .persist()

Then spark will shuffle only events RDD (which is smaller than userData RDD)

Following requires all key-value pairs with the same key on the same machine
Grouping is done using hash partitioner with default params
val purchasesPerCustomer=
  purchasesRdd.map( p => (p.customerId), p.price)) // Pair RDD
      .groupByKey()

How to know when shuffle may happened?
using function toDebugString to see the excecution plan
partitioned.reducedByKey((v1,v2) => (v1._1 + v2._1, v1._2 + v2._2))
   .toDebugString

Methods which might the cause the shuffle
cogroup
groupWith
join
leftOterJoin
rightOuterJoin
groupByKey - uses hash partitioner with default parametes
reduceByKey
combineByKey
distinct
intersection
repartition
coalesce

But if we use pre-partitioning then running reduceByKey on pre-partitioned RDD will
do reducing work locally; only final result will be send from worker to driver

Join called on 2 RDDs that a prepartitioned with same partitioner and cached on same machine
will work locally as well; nio shuffling

myRDD.toDebugString
repartition, join, cogroup and any of *By* or *ByKey transformaions can result in shuffle
if you declare numPartitions parameter it will probably shuffle
combineByKey groupByKey calls a shuffle
s
Skewed data solutions:
- Salting ( modify  Key=Foo to Foo+rendom.nextInt(sultNumber)
- Isolated Salting
- Isolated Map Joins


RDD is lazy, just DAG is bulding until the action is asked, like collect()
.cache()
.coalesce()  - ermove empty RDD?

ReduceByKey vs GroupByKey
TreeDeduce over Reduce
Use complex/nested types

Off-heap memory

<h3>RDD types</h3>

 hadoopRDD :  1 partition per HDFS block, dependencies->none,  compute(partition)=read corresponding block
 converted into MappedRDD

 filteredRDD: partitions: same as parent RDD; depndencies: one-to-one on parent; compute(partition)= compute parent and filter it
 JoinedRDD: 1 partition per redue task; dependencies: shuffle on each parent; compute(partition) read and join shuffled data; partitioner=HashPartitioner(numTasks)
 mappedRDD
 PairRDD
 ShuffleRDD
 UnionRDD
 PyrhonRDD
 DoubleRDD
 JdbcRDD
 JsonRDD
 SchemaRDD
 VertedRDD
 EdgeRDD
 --------
 CassandraRDD:
    va cassandraRDD=sc.cassandraTable("mykeyspace","columnA")  - predicate pushdown
 GeoRDD
 ExSpark

----
spark.cores.max
spark.executor.memory

--------
RSS.cache==RDD.persist(MEMORY_ONLY)  it is the same
RDD.persist(MEMORY_AND_DISK)
RDD.persist(DISK_ONLY)
RDD.persist(MEMORY_AND_DISK_SER)
Cryo serialiazation

spark.storage,memoryFraction:  shuffle memory, cached RDDs user programs

Hadoop means 3 Apache projects: HDFS, Map-Reduce, Yarn
Job Traker, Named Node
Spark is contender of Map-Reduce
conf.registrKryoClass()  instead Java Serialization to speed
High churn/Lost churn
array of ints instead of linkedlist
CuseConstMArkSweep

<h3>How to run Spark in Standalone client mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode client \
-master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in Standalone cluster mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode cluster \
-master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in YARN client mode</h3>
Yet Another Resoure Manager - YARN

Resource Manager manages many NodeManagers
Resource Manager  has Scheduler

spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode client \
-master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in YARN cluster mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode cluster \
-master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
</pre>

<h3> Spark +  Oozie</h3>
https://habrahabr.ru/post/338952/

</body>
