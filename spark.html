<head>
 <link rel="stylesheet" href="style.css">
</head>

<body>
<!--
<pre>
-->
<h2>Spark</h2>

<pre>
<h3>Pair RDD </h3>
 def groupByKey(): RDD[(K,Iterable[V])]
 def reduceByKey(func: (V,V) => V): RDD[(K,V)]
 def join[W](other: RDD[(K,W)]): RDD[(K,(V,W))]

 var pairRDD = rdd.map(page => (title, page.text))

 case class Event(organizer: String, name: String, budget: Int)   //key:organizer
 var eventsRDD = sc.parallelize(...)
     .map(event => (event.orgazier, event.budget))

 def groupByKey(): RDD[(K, Iterable[V])]

 var groupedRdd= eventsRDD.groupByKey()
 groupedRdd.collect().foreach(println)

reduceByKey: conceptionally it is combination of groupByKey and reducing all vals per key
def reduceByKey (func: (V,V) => V): RDD[(K,V)]
val budgetsRDD=eventsRDD.reduceByKey(_+_)
budgetsRdd.collect().foreach(println)

 def mapValues[U] (f: V=>U): RDD[(K,U)]
 rdd.map { case(x,y): (x,func(y))}  //applies func to only values in PairRDD

 def countByKey(): Map[K,Long]) // counts # of elements per key

 In regular Scala collections there is groupBy:
 def groupBy[K](f: A=>K): Map[K, Traversable[A]]
 Usage example:
 val ages=List(34,12,14,78)
 val grouped=ages.groupBy(
    age =>
    if (age >17) "adult"
    else if (age <50) "adult"
    else "senior"
 )

<h3>How to configure RDD persistance</h3>
 - in memory as Java objects
 - on disk as Java objects
 - in memory as serialized Java objects
 - on disk as serialized Java objects
 - both in memory an on disk
 cache()
 persist
 MEMORY_ONLY
 MEMORY_ONLY_SER
 MEMORY_AND_DISK
 MEMORY_AND_DISK_SER
 DISK_ONLY

<h3>How to run Spark in Standalone client mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode client \
-master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in Standalone cluster mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode cluster \
-master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in YARN client mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode client \
-master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10

<h3>How to run Spark in YARN cluster mode</h3>
spark-submit \
-class org.apache.spark.examples.SparkPi \
-deploy-mode cluster \
-master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
</pre>

</body>
