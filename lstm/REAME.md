## LSTM
```
 feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R^n.
  ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete.
  
Why do we need recurrent neural networks when we already have the beloved ANNs (and CNNs) in all their glory?
It boils down to a few things:

ANNs can’t deal with sequential or “temporal” data
ANNs lack memory
ANNs have a fixed architecture  


RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory.
The constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; 

``` 
 
<https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b>

<https://www.youtube.com/watch?v=EQ-JE38e8XE> ru

<https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi>

<https://www.asozykin.ru/courses/nnpython> 
<https://tproger.ru/translations/neural-network-zoo-2/>

<https://uzclip.net/video/1DNcYIN0aaA/%D0%BA%D0%B2%D0%B0%D1%80%D1%82%D0%B8%D1%80%D0%BD%D0%B8%D0%BA-deep-learning-%D0%BD%D0%B0-%D0%BF%D0%B0%D0%BB%D1%8C%D1%86%D0%B0%D1%85.html>

<http://mechanoid.kiev.ua/>

http://www.machinelearning.ru/wiki/images/7/78/2017_417_DrapakSN.pdf

<https://en.wikipedia.org/wiki/Long_short-term_memory>

<https://neural.wtf/floyd.html>

http://library.eltech.ru/files/vkr/2017/bakalavri/3382/2017%D0%92%D0%9A%D0%A0338212%D0%A4%D0%90%D0%9D%D0%98%D0%A4%D0%90%D0%A2%D0%AC%D0%95%D0%92%D0%90.pdf


<https://blog.heuritech.com/2016/01/20/attention-mechanism/> attention
