<head>
 <link rel="stylesheet" href="style.css">
</head>

<body>
	<h2>Machine Learning</h2>
<pre>
<a href=https://github.com/mlubinsky/mlubinsky.github.com/tree/master/ml>Machine Learning code snippets</a>

<a href=https://en.wikipedia.org/wiki/List_of_machine_learning_concepts>List of machine learning concepts</a>

<a href=http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/>Tour-of-machine-learning-algorithms</a>

 Visualization
https://github.com/PAIR-code/facets
https://github.com/JosPolfliet/pandas-profiling

<img src="ml_guide.svg" width="1200">
<img src="ml1.png">
<img src="ml2.png" width="1200" height="800">


 Bias is the difference between your model's expected predictions and the true values.
 The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.
 The error due to variance is taken as the variability of a model prediction for a given data point.
 The variance is how much the predictions for a given point vary between different realizations of the model.
 The small sample size is a source of variance. If we increased the sample size, the results would be more consistent.
 The results still might be highly inaccurate due to our large sources of bias, but the variance of predictions will be reduced
 Variance refers to your algorithm's sensitivity to specific sets of training data.

<img src="biasvariance.png" width="1200" height="800">

High bias, low variance: model are consistent but inaccurate on averag
High variance, low bias: model are inconsistent but accurate on average
Low variance tends to be related to simpler atgorithms (regression, naive bayes, linear, parametric)
Low bias tends to be related to complex atgorithms (Decision tree, Near Neigbour, Non-parametric)

Regression algo can be regularized to reduce complexity
Decision tree can be pruned to reduce complexity

Too complex model -> overfitting
Too simple model -> underfitting

The Linear model does not fit the data very well and is therefore said to have a higher bias than the polynomial model.

Overfitting:
---------------
Our model doesn’t generalize well from our training data to unseen data.
Cross-validation is a powerful preventative measure against overfitting.
K-fold cross-validation: partition the the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”).

- Remove feature
- Regularization:  you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.
- Early stopping
When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.
Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.
Early stopping refers stopping the training process before the learner passes that point.
<img src="early-stopping-graphic.jpg" width="600" height="400">


Underfitting
--------------
occurs when a model is too simple – informed by too few features or regularized too much – which makes it inflexible in learning from the dataset.

In both Machine Learning and Curve Fitting, you want to come up with a model that explains (fits) the data. However, the difference in the end goal is both subtle and profound.
In Curve Fitting, we have all the data available to us at the time of fitting the curve. We want to fit the curve as best as we can.
In Machine Learning, only a small set (the training set) of data is available at the time of training. We obviously want a model that fits the data well, but more importantly, we want the model to generalize to unseen data points



Classification is forecasting the target class / category
Regression if forecasting a value.

Logistic regression - dependent variable is categorical.
Logistic function predict the corresponding target class.
Probability of result = logistic function:
y = 1 / ( 1 + exp(-f(x)))  in range from 0 to 1.
if f(x) = 0 then y=0.5
if f(x) is big negative # then y=0
if f(x) is big positive # then y=1

f(x) = ax+b, here X is the input vector and A is a parameter vector
Goal is to find A.
The common method is Max likelehood (logarithm) criteria; the gradiend descend can be used

x - random outcomes
theta - parameter
L(theta| x) = P(x | theta)

Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood in maximum likelihood estimation

Regulariation: to decrease overfitting
L2:
L1

Bagging and other resampling techniques can be used to reduce the variance in model predictions.
In bagging (Bootstrap Aggregating), numerous replicates of the original data set are created using random selection with replacement.
Each derivative data set is then used to construct a new model and the models are gathered together into an ensemble.
To make a prediction, all of the models in the ensemble are polled and their results are averaged.
Bagging attempts to reduce the chance overfitting complex models.

It trains a large number of "strong" learners in parallel.
A strong learner is a model that's relatively unconstrained.
Bagging then combines all the strong learners together in order to "smooth out" their predictions.
Boosting attempts to improve the predictive flexibility of simple models.

It trains a large number of "weak" learners in sequence.
A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).
Each one in the sequence focuses on learning from the mistakes of the one before it.
Boosting then combines all the weak learners into a single strong learner.
While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.

Bagging uses complex base models and tries to "smooth out" their predictions, while boosting uses simple base models and tries to "boost" their aggregate complexity.




Decision Tree:  https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/
The primary challenge in the decision tree implementation is to identify which attributes do we need to consider as the root node and each level.
In decision tree algorithm calculating the nodes and forming the rules will happen using the information gain and Gini index.
Information Gain calculates the expected reduction in entropy due to sorting on the attribute.
Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified. It means an attribute with lower gini index should be preferred.


Random forest algorithm is a supervised classification algorithm.
Random forest algorithm can use both for classification and the regression kind of problems.
Random Forests works by training numerous decision trees each based on a different resampling of the original training data.
In Random Forests the bias of the full model is equivalent to the bias of a single decision tree (which itself has high variance).
By creating many of these trees, in effect a "forest", and then averaging them the variance of the final model can be greatly reduced over that of a single tree. In practice the only limitation on the size of the forest is computing time as an infinite number of trees could be trained without ever increasing bias and with a continual (if asymptotically declining) decrease in the variance.


Separate training and test sets
------------------------------------
Split the data into three sets — training (60%), validation (a.k.a development) (20%) and test (20%).
Use the training set to train different models, the validation set to select a model
and finally report performance on the test set.

- Trying appropriate algorithms (No Free Lunch)
- Fitting model parameters
- Tuning impactful hyperparameters
- Proper performance metrics
- Systematic cross-validation

<img src="ml/ml.png" width="800" height="1400">


<a href=https://spandan-madan.github.io/DeepLearningProject/>ML from start to end</a>
http://pbpython.com/categorical-encoding.html
https://github.com/onurakpolat/awesome-analytics
<a href=https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5>ML cheetsheets</a>
<a href=http://scikit-learn.org/stable/tutorial/>Scikit-learn</a>
<a href=https://github.com/vahidk/EffectiveTensorflow>Tensor Flow</a>
<a href=http://dlib.net/>ML DLIB C++</a>
https://machinelearning.apple.com/2017/08/06/siri-voices.html
https://medium.com/@mngrwl/explained-simply-how-deepmind-taught-ai-to-play-video-games-9eb5f38c89ee
<a href=https://github.com/likedan/Awesome-CoreML-Models>CoreML</a>
<a href=https://jixta.wordpress.com/>ML blog</a>
<a href=https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur#>ML method</a>
<a href=https://www.analyticsvidhya.com/blog/2017/01/the-most-comprehensive-data-science-learning-plan-for-2017/>ML plan</a>
<a href=http://mloss.org/software/>MLOSS.org</a>
<a href=https://arxiv.org/pdf/1512.02595.pdf>Speech recognition</a>
<a href=http://distill.pub/>distill.pub</a>
<a href=https://habrahabr.ru/company/ods/blog/325654/#affinity-propagation>PCA 1</a>
<a href=https://blog.3blades.io/principal-component-analysis-pca-a-practical-example-68d8df12c4bf>PCA 2</a>

<a href=ml/QnA.html>Q & A</a>

https://habrahabr.ru/post/311092/   standard distibutions
http://www.datatau.com/
https://www.bonaccorso.eu/
https://morfizm.livejournal.com/1136917.html  BitFunnel
https://blog.statsbot.co/
http://rpubs.com/JDAHAN/172473
http://www.datasciencecentral.com/profiles/blogs/21-great-articles-and-tutorials-on-time-series
https://jakevdp.github.io/PythonDataScienceHandbook/  BOOK ONLINE
https://habrahabr.ru/post/337028/  video bayes deep ML
https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
https://medium.freecodecamp.org/statistical-inference-showdown-the-frequentists-vs-the-bayesians-4c1c986f25de
https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-algorithm-choice
http://www.datasciencecentral.com/profiles/blogs/under-the-hood-with-reinforcement-learning-understanding-basic-rl
http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/
https://elitedatascience.com/machine-learning-iteration
https://elitedatascience.com/dimensionality-reduction-algorithms
https://elitedatascience.com/machine-learning-algorithms
</pre>
</body>
