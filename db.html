<head>
<link rel="stylesheet" href="style.css">
</head>


<pre>
https://dbdiagram.io/home . DB schema diargrams
http://www.redbook.io/
http://coding-geek.com/how-databases-work/
https://yunpengn.github.io/blog/
https://dbdb.io/
http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/

https://www.cockroachlabs.com/blog/vectorized-hash-joiner/
https://numeracy.co/blog/life-of-a-sql-query
http://www.sommarskog.se/query-plan-mysteries.html

https://habr.com/company/psb/blog/434730/ . In memory DBs

https://habr.com/ru/company/oleg-bunin/blog/319968/ . Tarantool, NoSQL
https://habr.com/ru/company/oleg-bunin/blog/413557/ .   NoSQL

https://orientdb.com/  . OrientDB
https://habr.com/ru/post/324012/  . OrientDB
https://habr.com/ru/company/orienteer/blog/324758/ . OrientDB

https://habr.com/ru/post/440306/ Проблемы масштабирования БД в высоконагруженных системах

https://habr.com/ru/company/otus/blog/451042/ . Postgres vs Cassandra vs Mongo 
 
<h2>MVCC (MultiVersion Concurrency Control)</h2>
https://en.wikipedia.org/wiki/Multiversion_concurrency_control
The simplest way is to make all readers wait until the writer is done, which is known as a read-write lock. 
Locks are known to create contention especially between long read transactions and update transactions. 
MVCC aims at solving the problem by keeping multiple copies of each data item. 
In this way, each user connected to the database sees a snapshot of the database at a particular instant in time. 
Any changes made by a writer will not be seen by other users of the database until the changes have been completed 
(or, in database terms: until the transaction has been committed.)

When an MVCC database needs to update a piece of data, it will not overwrite the original data item with new data, 
but instead creates a newer version of the data item. Thus there are multiple versions stored. 
The version that each transaction sees depends on the isolation level implemented. 
The most common isolation level implemented with MVCC is snapshot isolation. 
With snapshot isolation, a transaction observes a state of the data as when the transaction started. 
MVCC introduces the challenge of how to remove versions that become obsolete and will never be read. In some cases, 
a process to periodically sweep through and delete the obsolete versions is implemented. 
This is often a stop-the-world process that traverses a whole table and rewrites it with the last version of each data item. 


https://en.wikipedia.org/wiki/Multiversion_concurrency_control
http://elliot.land/post/sql-transaction-isolation-levels-explained

If someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data.
There are several ways of solving this problem, known as concurrency control methods.
The simplest way is to make all readers wait until the writer is done, which is known as a lock.
This can be very slow, so MVCC takes a different approach: each user connected to the database sees a snapshot of the database at a particular instant in time.
Any changes made by a writer will not be seen by other users of the database until the changes have been completed
(or, in database terms: until the transaction has been committed.)


The SQL standard tried to define four isolation levels (read uncommitted, read committed, repeatable read and serializable)
- Read uncommitted permits dirty reads, non repeatable reads and phantom reads.
- Read committed permits non repeatable reads and phantom reads.
- Repeatable read permits only phantom reads.
- Serializable does not permit any read errors.    Client perform transactions in serial.

<h2>Sharding</h2>
https://www.citusdata.com/blog/2017/08/09/principles-of-sharding-for-relational-databases/
https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6
https://habr.com/company/oleg-bunin/blog/433370/
https://www.quora.com/What-is-the-difference-between-replication-partitioning-clustering-and-sharding

If there is a small table which is part of SQL join with a sharded table then make a replica of small table to all nodes.
Consistent hashing.
 Rendezvous hashing, Highest Random Weight : shard_id = arg max hash(object_id, shard_id).

https://habr.com/company/gridgain/blog/430852/ . ACID consistency distributed DBs
https://medium.com/capital-one-tech/batch-and-streaming-in-the-world-of-data-science-and-data-engineering-2cc029cdf554
https://www.youtube.com/channel/UCHnBsf2rH-K7pn09rb3qvkA

https://harelba.github.io/q/ q allows performing SQL-like statements on tabular text data. 
https://news.ycombinator.com/item?id=18453133 SQL on CSV

<h2> Apache Flink</h2>
https://www.infoq.com/presentations/sql-streaming-apache-flink

<h2>Prometheus - for system monitoring </h2>
https://habr.com/ru/company/southbridge/blog/455290/
 https://habr.com/ru/company/itsumma/blog/350200/
В состав Prometheus входят следующие компоненты:
* сервер, который считывает метрики и сохраняет их в темпоральной (time series) базе данных;
* клиентские библиотеки для различных языков программирования (Go, Java, Python, Ruby, ...)
* Pushgateway — компонент для приёма метрик кратковременных процессов;
* PROMDASH — дашборд для метрик;
* инструменты для экспорта данных из сторонних приложений (Statsd, Ganglia, HAProxy и других);
* менеджер уведомлений AlertManager (на текущий момент находится на стадии бета-тестирования);
* клиент командной строки для выполнения запросов к данным.

Большинство из них написаны на Go, а совсем небольшая часть — на Ruby и Java. 

Все компоненты Prometheus взаимодействуют между собой по протоколу HTTP:

Сбор метрик в Prometheus осуществляется с помощью механизма pull. 
Имеется также возможность сбора метрик с помощью механизма push (для этого используется специальный компонент pushgateway, 
который устанавливается отдельно). Это может понадобиться в ситуациях, 
когда сбор метрики с помощью pull по тем или иным причинам невозможен: например, при наблюдении за сервисами, защищёнными фаерволлом. Также механизм push может оказаться полезным при наблюдении за сервисами, 
подключающихся к сети периодически и на непродолжительное время.
 
Prometheus использует pull модель сбора метрик: у него есть список экспортеров и он опрашивает их по HTTP, 
собирая с них список метрик и кладя их к себе в хранилище.


Экспортер — это агент, который занимается сбором метрик непосредственно с сущности 
(сервера в целом, или конкретного приложения), которую надо мониторить. 
У Prometheus богатые возможности для инструментации, поэтому экспортеры есть для большинства популярных приложений, 
и написать свой в случае надобности не представляет особого труда.

https://habr.com/ru/post/345370/
postgres_exporter работает следующим образом: он подключается к PostgreSQL, выполняет запросы к служебным таблицам 
и выставляет результаты в специальном формате с помощью внутреннего HTTP-сервера для забора их Prometheus'ом. 
Важный момент: помимо большого набора дефолтных запросов, можно определить свои и собирать любые данные, 
которые можно получить с помощью SQL, включая какие-нибудь бизнес-метрики. 
 
https://habr.com/ru/company/selectel/blog/275803/
https://habr.com/ru/company/otus/blog/358588/

https://habr.com/ru/post/441136/ .  How to store metrics for a long time
https://habr.com/ru/company/funcorp/blog/445370/


Writing: Скорость накопления данных стремится к стабильной величине: обычно сервисы, которые вы мониторите, 
посылают примерно одинаковое количество метрик, и инфраструктура меняется относительно медленно. 

Integration with Grafana
https://grafana.com/dashboards/7901

http://micrometer.io/ . JVM Application Monitoring
https://habr.com/ru/post/442080/

https://habr.com/ru/post/420633/ . GeoIP plotting (WorldMap)
https://habr.com/ru/post/412897/

<h2>DB Mongo </h2>

https://www.scaleapi.com/blog/athena#asdf
https://news.ycombinator.com/item?id=17438516
<h2>HANA</h2>
https://habr.com/ru/article/436462/

<h2>DB Internals </h2>
https://www.cs.purdue.edu/homes/rompf/papers/tahboub-sigmod18.pdf . Query Compiler
https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database .  LMDB
https://habr.com/company/oleg-bunin/blog/358984/
https://news.ycombinator.com/item?id=17190947  database internals

2-phase commit
https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/

<h2>Time Series DB</h2>
https://news.ycombinator.com/item?id=18553336
https://news.ycombinator.com/item?id=18402890
https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-i-ec2b6d44ba39
https://medium.com/@Pinterest_Engineering/goku-building-a-scalable-and-high-performant-time-series-database-system-a8ff5758a181

https://thecuriousdev.org/logstash-elk-stack-tutorial-part-1/ Logstash

http://richardstartin.uk/how-a-bitmap-index-works/

https://movio.co/en/blog/improving-with-sql-and-charts/  SQL to plot
https://simonwillison.net/2018/Jan/17/datasette-publish/	datasette
https://news.ycombinator.com/item?id=16170892               datasette
https://habrahabr.ru/post/346884/ in memory

https://habrahabr.ru/company/ruvds/blog/350310/  Caching

https://habrahabr.ru/company/mailru/blog/267469/   type-ahead hints in dropdown
http://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D0%BE%D1%80
https://thomaswdinsmore.com/2017/02/01/year-in-sql-engines/
https://www.pilosa.com/docs/latest/introduction/  distributed bitmap index
https://ordepdev.me/posts/what-you-should-know-about-database-storage-and-retrieval

<h2>ORACLE </h2>
https://livesql.oracle.com
https://blogs.oracle.com/sql/how-to-convert-rows-to-columns-and-back-again-with-sql-aka-pivot-and-unpivot
select TABLE_NAME from user_tables

select * from sometable where rownum <= 10 order by name

select * from
( select * from emp   order by sal desc )
where ROWNUM <= 5;

<h2>SQL</h2>
 https://www.interviewbit.com/sql-interview-questions/
 
<h2>Hierarhy</h2> 
 https://stackoverflow.com/questions/4048151/what-are-the-options-for-storing-hierarchical-data-in-a-relational-database
 https://news.ycombinator.com/item?id=20027586 Hierarchy and RECURSIVE SQL
 https://github.com/bitnine-oss/agensgraph . Postgres extension  AgensGraph
 http://patshaughnessy.net/2017/12/13/saving-a-tree-in-postgres-using-ltree
 
 WITH RECURSIVE cte (id, message, author, path, parent_id, depth)  AS (
  SELECT  id,
          message,
          author,
          array[id] AS path,
          parent_id,
          1 AS depth
  FROM    comments
  WHERE   parent_id IS NULL

  UNION ALL
 
  SELECT  comments.id,
          comments.message,
          comments.author,
          cte.path || comments.id,
          comments.parent_id,
          cte.depth + 1 AS depth
  FROM    comments
          JOIN cte ON comments.parent_id = cte.id
  )
  SELECT id, message, author, path, depth FROM cte
  ORDER BY path;
 
https://www.youtube.com/watch?v=swR33jIhW8Q
https://habr.com/ru/post/448072/ . Joins
https://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem/
https://habr.com/post/422461/ . examples of SQL with answers
http://www.sql-workbench.eu/dbms_comparison.html

Differennce between ON and WHERE clause:

https://blog.jooq.org/2019/04/09/the-difference-between-sqls-join-on-clause-and-the-where-clause/

CREATE TABLE A( i int, val int,  data varchar(16) );
CREATE TABLE B( i int, val int,  data varchar(16) );

INSERT INTO A values(1, 10, '2010-09-05');
INSERT INTO A values(2, 20, '2010-10-05');
INSERT INTO A values(3, 30, '2010-10-05');
INSERT INTO A values(3, 40, '2010-10-05');

INSERT INTO B values(2, 201, '2010-09-05');
INSERT INTO B values(2, 202, '2020-10-05');
INSERT INTO B values(3, 300, '2010-10-05');

select A.i, A.data, B.i, B.data from A LEFT JOIN B
ON A.i=B.i WHERE A.data='2010-10-05' and B.data='2010-10-05' ;
+------+------------+------+------------+
| i    | data       | i    | data       |
+------+------------+------+------------+
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    3 | 2010-10-05 |    3 | 2010-10-05 |
+------+------------+------+------------+
2 rows in set (0.00 sec)

select A.i, A.data, B.i, B.data from A LEFT JOIN B
ON ( A.i=B.i and A.data='2010-10-05' and B.data='2010-10-05') ;
+------+------------+------+------------+
| i    | data       | i    | data       |
+------+------------+------+------------+
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    1 | 2010-09-05 | NULL | NULL       |
|    2 | 2010-10-05 | NULL | NULL       |
+------+------------+------+------------+
4 rows in set (0.00 sec)


https://cs.uwaterloo.ca/~plragde/flaneries/FDS/ functional data structures



http://www.mysqltutorial.org/mysql-row_number/

https://news.ycombinator.com/item?id=13517490

https://vadimtropashko.wordpress.com/%e2%80%9csql-design-patterns%e2%80%9d-book/about/

<h2>Running total</h2>
select
    a.date,
    sum(b.sales) as cumulative_sales
from sales_table a
join sales_table b on a.date >= b.date
group by a.date
order by a.date;

Using window function:
select
    date,
    sum(sales) over (order by date rows unbounded preceding) as cumulative_sales
from sales_table;

<h2>Window functions</h2>
https://oracle-base.com/articles/misc/analytic-functions
https://hashrocket.com/blog/posts/sql-window-functions
https://www.fromdual.com/mariadb-10-2-window-function-examples
https://blog.jooq.org/2014/04/29/nosql-no-sql-how-to-calculate-running-totals/
https://blog.jooq.org/2013/11/03/probably-the-coolest-sql-feature-window-functions/

SELECT
  employee_name,
  employee_id,
  salary,
  rank() OVER(PARTITION BY dept ORDER BY salary DESC),
  100.0*salary/sum(salary) OVER (PARTITION BY dept)
FROM employee;

http://www.windowfunctions.com/

CREATE TABLE cats(
   name varchar(10),
   breed varchar(10),
   weight float,
   color varchar(10),
   age int
);

Q1 RUNNING TOTAL:
The cats must by ordered by name and will enter an elevator one by one. We would like to know what the running total weight is.
=================
select name, sum(weight)
over (order by name) as running_total_weight
from cats order by name


Q2: Partitioned Running Totals
==================================
The cats must by ordered first by breed and second by name. They are about to enter an elevator one by one. When all the cats of the same breed have entered they leave.

We would like to know what the running total weight of the cats is.


select name, breed, sum(weight)
over (partition by breed order by name) as running_total_weight from cats


Q3: Counting
==========
The cats form a line grouped by color. Inside each color group the cats order themselves by name. Every cat must have a unique number for its place in the line.

We must assign each cat a unique number while maintaining their color & name ordering.

select row_number() over (order by color,name) as unique_number, name, color from cats


Q4: Ordering
==================
We would like to find the fattest cat. Order all our cats by weight.

The two heaviest cats should both be 1st. The next heaviest should be 3rd.

Return: ranking, weight, name
Order by: ranking, name desc


select rank() over (order by weight desc) as ranking, weight, name
from cats order by ranking, name DESC


Q5: Quartiles
==============
We are worried our cats are too fat and need to diet.
We would like to group the cats into quartiles by their weight.
Return: name, weight, weight_quartile
Order by: weight

select name, weight, ntile(4) over ( order by weight) as weight_quartile from cats order by weight_quartile, name


Q6: Ranking
=====================
For cats age means seniority, we would like to rank the cats by age (oldest first).
However we would like their ranking to be sequentially increasing.
Return: ranking, name, age
Order by: ranking, name
select dense_rank() over (order by age DESC) as r, name,age from cats order by r, name

Q7 Compare to Previous Row
==============================
Cats are fickle. Each cat would like to lose weight to be the equivalent weight of the cat weighing just less than it.

Print a list of cats, their weights and the weight difference between them and the nearest lighter cat ordered by weight.

Return: name, weight, weight_to_lose
Order by: weight

select name, weight, coalesce(weight - lag(weight, 1) over (order by weight), 0) as weight_to_lose FROM cats order by weight


Q8: Compare to Previous Row Part 2
=================================
The cats now want to lose weight according to their breed. Each cat would like to lose weight to be the equivalent weight of the cat in the same breed weighing just less than it.

Print a list of cats, their breeds, weights and the weight difference between them and the nearest lighter cat of the same breed.

Return: name, breed, weight, weight_to_lose
Order by: weight

select name, breed, weight, coalesce(weight - lag(weight, 1) over (partition by breed order by weight), 0) as weight_to_lose from cats order by weight, name


Q9: First of each Group
==========================
Cats are vain. Each cat would like to pretend it has the lowest weight for its color.

Print cat name, color and the minimum weight of cats with that color.

Return: name, color, lowest_weight_by_color
Order by: color, name

select name, color, first_value(weight) over (partition by color order by weight) as lowest_weight_by_color from cats order by color, name


Q10: Using the Window clause
===============================
This SQL function can be made simpler by using the WINDOW statement. Please try and use the WINDOW clause.

Each cat would like to see what half, third and quartile they are in for their weight.

Return: name, weight, by_half, thirds, quartile
Order by: weight

select name, weight, ntile(2) over ntile_window as by_half, ntile(3) over ntile_window as thirds, ntile(4) over ntile_window as quart from cats window ntile_window AS ( ORDER BY weight)







<h2>Links</h2>
http://www.olapcube.com/mdxhelp/
http://knexjs.org/

<b>Time Series Databases<b>
http://db.cs.cmu.edu/seminar2017/   Time Series Databases
http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html

<h2>SQLite</h2>
http://tech.marksblogg.com/sqlite3-tutorial-and-guide.html
http://charlesleifer.com/blog/going-fast-with-sqlite-and-python/
https://github.com/simonw/csvs-to-sqlite
https://simonwillison.net/2017/Nov/13/datasette/


<h2>MySQL</h2>
https://habrahabr.ru/post/351740/  schema performance
https://news.ycombinator.com/item?id=15613856
MySQL variables:
set @n:=-1;
select (select @n:= @n+1) n
from MyTable limit 60






<h2>Cassandra</h2>
https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc
https://medium.com/techlogs/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8
http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html

Issue with following table - the partotion become very big very fast.
CREATE TABLE raw_data (
    sensor text,
    ts timeuuid,
    readint int,
    primary key(sensor, ts)
) WITH CLUSTERING ORDER BY (ts DESC) 
  AND compaction = {'class': 'TimeWindowCompactionStrategy', 
                    'compaction_window_size': 1, 
                    'compaction_window_unit': 'DAYS'};
            
Let change primary key:
one partition per sensor per day would be a good choice if we’re storing 50-75MB of data per day. 
We could just as easily use week (starting from some epoch), or month and year as long as the partitions stay under 100MB.

CREATE TABLE raw_data_by_day (
sensor text,
day text,
ts timeuuid,
reading int,
primary key((sensor, day), ts)
) WITH CLUSTERING ORDER BY (ts DESC) 
       AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', 
                     'compaction_window_unit': 'DAYS', 
                     'compaction_window_size': 1};
            
append-only store
compaction tumbstone (deleted marker)

Bloom filter garanteed true negatives

TimeWindowCompaction
SizeTieredCompaction
LeveledCompaction

materialed view
keyspace: bag of tables with replication factor, similar to schema
class: SimpleStategy or NetworkStrategy

Cassandra: up to 10Tb per node, HDFS has better density

rack - aware, region - aware
partition key is subset of Primary Key
Columns and tables support an optional expiration period called TTL (time-to-live); TTL is not supported on counter columns. Define the TTL value in seconds. Data expires once it exceeds the TTL period and is then marked with a tombstone. Expired data continues to be available for read requests during the grace period, see gc_grace_seconds. Normal compaction and repair processes automatically remove the tombstone data.


mirmur3_hash(primary_key)  -> token   (range of keys)
Keyspace attributes:
- replication factor
- replica placement strategy
- column family for keyspace

cqlsh> describe keyspaces
select columnfamily_name from system.schema_columnfamilies where keyspace_name = 'test';
or
cqlsh> use products;
cqlsh:products> describe tables;    or DESCRIBE COLUMNFAMILIES;

if nodes are non in sync then node with latest data wins
http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html
Consistecy levels (per statement read/write) for reads and writes:
ALL - all replicas ack
QUORUME > 51%
LOCAL_QUORUM in local DC
ONE
TWO


write:  commit log and to memtable -> SSTable
cassandra.yml    fsynced every 10s


http://abiasforaction.net/cassandra-query-language-cql-tutorial/

https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/

https://www.instaclustr.com/resource/cassandra-nosql-data-model-design/
https://www.instaclustr.com/cassandra-nosql-data-model-design-2/
https://www.instaclustr.com/apache-cassandra-3-x-and-materialized-views/

https://tekslate.com/cassandra-interview-questions-and-answers

https://opencredo.com/how-not-to-use-cassandra-like-an-rdbms-and-what-will-happen-if-you-do/
https://opencredo.com/cassandra-data-modelling-patterns/
https://opencredo.com/training/cassandra-fundamentals-data-modelling/
https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04
https://vitobotta.com/2017/04/02/getting-started-apache-cassandra/

http://cassandra.alteroot.org/
http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html
http://lucasmanual.com/blog/quick-intro-to-cassandra-vs-mongodb-with-python/
http://blogs.shephertz.com/2015/04/22/why-cassandra-excellent-choice-for-realtime-analytics-workload/
https://opencredo.com/spark-testing/

https://www.youtube.com/watch?v=Mvsy2DZhKSw
https://www.youtube.com/watch?v=HuDJBTPdaOA
https://www.reddit.com/r/cassandra/comments/6l01iu/wide_row_append_only_timeseries_data_no_ttl_what/





https://www.instaclustr.com/resource/apache-spark-apache-cassandra-powering-intelligent-applications/
https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/
log-structured merge trees (LSMT)
LSM trees  just dumping stuff at the end of an array. However this means you have to scan that array to do lookups (as opposed to something easier like binary search). Then as your array gets big, you merge and flush it down to a lower "layer" of the lsm tree which is slightly bigger and sorted. And when that one fills, you flush further. And these merge-flushes are nice big sequential writes so that's nice too.
http://neovintage.org/2016/04/07/data-modeling-in-cassandra-from-a-postgres-perspective/

https://www.goetas.com/blog/how-i-approached-software-development-and-why-i-prefer-postgresql-to-mysql/

<h2>TimeSeries</h2>
https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-i-ec2b6d44ba39
https://blog.outlyer.com/top10-open-source-time-series-databases
https://db.cs.cmu.edu/seminar2017/
https://oss.redislabs.com/redistimeseries/

https://github.com/sirixdb/sirix . SirixDB

http://devconnected.com/4-best-time-series-databases-to-watch-in-2019/
https://news.ycombinator.com/item?id=19825566
https://news.ycombinator.com/item?id=15649405

<h2>Postgres</h2>
https://blog.timescale.com/scaling-partitioning-data-postgresql-10-explained-cd48a712a9a1
http://www.craigkerstiens.com/categories/postgresql/
https://severalnines.com/blog/understanding-and-reading-postgresql-system-catalog
https://rob.conery.io/2018/08/01/simple-monthly-reports-in-postgresql-using-generate_series/   
https://habrahabr.ru/post/340460/
https://10clouds.com/blog/postgresql-10/
https://news.ycombinator.com/item?id=15634953
http://vvvvalvalval.github.io/posts/using-postgresql-temporary-views-for-expressing-business-logic.html
https://severalnines.com/blog/using-kubernetes-deploy-postgresql


<h2>Analytics</h2>
https://www.analyticsvidhya.com/blog/2014/09/commonly-asked-puzzles-analytics-interviews/
https://www.reddit.com/r/predictiveanalytics/
https://dzone.com/big-data-analytics-tutorials-tools-news
https://dzone.com/articles/streamline-the-machine-learning-process-using-apac
https://medium.com/rv-data/mleap-providing-near-real-time-data-science-with-apache-spark-c34e7df093ca
http://quasar-analytics.org/   Scala engine for analytics
</pre>
