
<head>
<link rel="stylesheet" href="style.css">
</head>

<pre>
https://thecuriousdev.org/logstash-elk-stack-tutorial-part-1/ Logstash

http://richardstartin.uk/how-a-bitmap-index-works/

https://simonwillison.net/2018/Jan/17/datasette-publish/	datasette
https://news.ycombinator.com/item?id=16170892               datasette
https://habrahabr.ru/post/346884/ in memory

https://habrahabr.ru/company/ruvds/blog/350310/  Caching

https://habrahabr.ru/company/mailru/blog/267469/   type-ahead hints in dropdown
http://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D0%BE%D1%80
https://thomaswdinsmore.com/2017/02/01/year-in-sql-engines/
https://www.pilosa.com/docs/latest/introduction/  distributed bitmap index

<h2>ORACLE </h2>
https://livesql.oracle.com
https://blogs.oracle.com/sql/how-to-convert-rows-to-columns-and-back-again-with-sql-aka-pivot-and-unpivot
select TABLE_NAME from user_tables

select * from sometable where rownum <= 10 order by name

select * from
( select * from emp   order by sal desc )
where ROWNUM <= 5;

<h2>SQL</h2>
Differennce between ON and WHERE clause:
CREATE TABLE A( i int, val int,  data varchar(16) );
CREATE TABLE B( i int, val int,  data varchar(16) );

INSERT INTO A values(1, 10, '2010-09-05');
INSERT INTO A values(2, 20, '2010-10-05');
INSERT INTO A values(3, 30, '2010-10-05');
INSERT INTO A values(3, 40, '2010-10-05');

INSERT INTO B values(2, 201, '2010-09-05');
INSERT INTO B values(2, 202, '2020-10-05');
INSERT INTO B values(3, 300, '2010-10-05');

select A.i, A.data, B.i, B.data from A LEFT JOIN B
ON A.i=B.i WHERE A.data='2010-10-05' and B.data='2010-10-05' ;
+------+------------+------+------------+
| i    | data       | i    | data       |
+------+------------+------+------------+
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    3 | 2010-10-05 |    3 | 2010-10-05 |
+------+------------+------+------------+
2 rows in set (0.00 sec)

select A.i, A.data, B.i, B.data from A LEFT JOIN B
ON ( A.i=B.i and A.data='2010-10-05' and B.data='2010-10-05') ;
+------+------------+------+------------+
| i    | data       | i    | data       |
+------+------------+------+------------+
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    3 | 2010-10-05 |    3 | 2010-10-05 |
|    1 | 2010-09-05 | NULL | NULL       |
|    2 | 2010-10-05 | NULL | NULL       |
+------+------------+------+------------+
4 rows in set (0.00 sec)


http://www.mysqltutorial.org/mysql-row_number/

<h2>Running total</h2>
select
    a.date,
    sum(b.sales) as cumulative_sales
from sales_table a
join sales_table b on a.date >= b.date
group by a.date
order by a.date;

Using window function:
select
    date,
    sum(sales) over (order by date rows unbounded preceding) as cumulative_sales
from sales_table;

<h2>Window functions</h2>
https://oracle-base.com/articles/misc/analytic-functions
https://hashrocket.com/blog/posts/sql-window-functions
https://www.fromdual.com/mariadb-10-2-window-function-examples
https://blog.jooq.org/2014/04/29/nosql-no-sql-how-to-calculate-running-totals/
https://blog.jooq.org/2013/11/03/probably-the-coolest-sql-feature-window-functions/

SELECT
  employee_name,
  employee_id,
  salary,
  rank() OVER(PARTITION BY dept ORDER BY salary DESC),
  100.0*salary/sum(salary) OVER (PARTITION BY dept)
FROM employee;

http://www.windowfunctions.com/

CREATE TABLE cats(
   name varchar(10),
   breed varchar(10),
   weight float,
   color varchar(10),
   age int
);

Q1 RUNNING TOTAL:
The cats must by ordered by name and will enter an elevator one by one. We would like to know what the running total weight is.
=================
select name, sum(weight)
over (order by name) as running_total_weight
from cats order by name


Q2: Partitioned Running Totals
==================================
The cats must by ordered first by breed and second by name. They are about to enter an elevator one by one. When all the cats of the same breed have entered they leave.

We would like to know what the running total weight of the cats is.


select name, breed, sum(weight)
over (partition by breed order by name) as running_total_weight from cats


Q3: Counting
==========
The cats form a line grouped by color. Inside each color group the cats order themselves by name. Every cat must have a unique number for its place in the line.

We must assign each cat a unique number while maintaining their color & name ordering.

select row_number() over (order by color,name) as unique_number, name, color from cats


Q4: Ordering
==================
We would like to find the fattest cat. Order all our cats by weight.

The two heaviest cats should both be 1st. The next heaviest should be 3rd.

Return: ranking, weight, name
Order by: ranking, name desc


select rank() over (order by weight desc) as ranking, weight, name
from cats order by ranking, name DESC


Q5: Quartiles
==============
We are worried our cats are too fat and need to diet.
We would like to group the cats into quartiles by their weight.
Return: name, weight, weight_quartile
Order by: weight

select name, weight, ntile(4) over ( order by weight) as weight_quartile from cats order by weight_quartile, name


Q6: Ranking
=====================
For cats age means seniority, we would like to rank the cats by age (oldest first).
However we would like their ranking to be sequentially increasing.
Return: ranking, name, age
Order by: ranking, name
select dense_rank() over (order by age DESC) as r, name,age from cats order by r, name

Q7 Compare to Previous Row
==============================
Cats are fickle. Each cat would like to lose weight to be the equivalent weight of the cat weighing just less than it.

Print a list of cats, their weights and the weight difference between them and the nearest lighter cat ordered by weight.

Return: name, weight, weight_to_lose
Order by: weight

select name, weight, coalesce(weight - lag(weight, 1) over (order by weight), 0) as weight_to_lose FROM cats order by weight


Q8: Compare to Previous Row Part 2
=================================
The cats now want to lose weight according to their breed. Each cat would like to lose weight to be the equivalent weight of the cat in the same breed weighing just less than it.

Print a list of cats, their breeds, weights and the weight difference between them and the nearest lighter cat of the same breed.

Return: name, breed, weight, weight_to_lose
Order by: weight

select name, breed, weight, coalesce(weight - lag(weight, 1) over (partition by breed order by weight), 0) as weight_to_lose from cats order by weight, name


Q9: First of each Group
==========================
Cats are vain. Each cat would like to pretend it has the lowest weight for its color.

Print cat name, color and the minimum weight of cats with that color.

Return: name, color, lowest_weight_by_color
Order by: color, name

select name, color, first_value(weight) over (partition by color order by weight) as lowest_weight_by_color from cats order by color, name


Q10: Using the Window clause
===============================
This SQL function can be made simpler by using the WINDOW statement. Please try and use the WINDOW clause.

Each cat would like to see what half, third and quartile they are in for their weight.

Return: name, weight, by_half, thirds, quartile
Order by: weight

select name, weight, ntile(2) over ntile_window as by_half, ntile(3) over ntile_window as thirds, ntile(4) over ntile_window as quart from cats window ntile_window AS ( ORDER BY weight)




<h2>MVCC (MultiVersion Concurrency Control)</h2>
https://en.wikipedia.org/wiki/Multiversion_concurrency_control
http://elliot.land/post/sql-transaction-isolation-levels-explained

If someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data.
There are several ways of solving this problem, known as concurrency control methods.
The simplest way is to make all readers wait until the writer is done, which is known as a lock.
This can be very slow, so MVCC takes a different approach: each user connected to the database sees a snapshot of the database at a particular instant in time.
Any changes made by a writer will not be seen by other users of the database until the changes have been completed
(or, in database terms: until the transaction has been committed.)


The SQL standard tried to define four isolation levels (read uncommitted, read committed, repeatable read and serializable)
- Read uncommitted permits dirty reads, non repeatable reads and phantom reads.
- Read committed permits non repeatable reads and phantom reads.
- Repeatable read permits only phantom reads.
- Serializable does not permit any read errors.    Client perform transactions in serial.
<h2>Links</h2>
https://www.citusdata.com/blog/2017/08/09/principles-of-sharding-for-relational-databases/
http://www.olapcube.com/mdxhelp/
http://knexjs.org/
http://db.cs.cmu.edu/seminar2017/   Time Series Databases

<h2>SQLite</h2>
http://tech.marksblogg.com/sqlite3-tutorial-and-guide.html
http://charlesleifer.com/blog/going-fast-with-sqlite-and-python/
https://github.com/simonw/csvs-to-sqlite
https://simonwillison.net/2017/Nov/13/datasette/


<h2>MySQL</h2>
https://news.ycombinator.com/item?id=15613856
MySQL variables:
set @n:=-1;
select (select @n:= @n+1) n
from MyTable limit 60






<h2>Cassandra</h2>
https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc
https://medium.com/techlogs/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8
append-only store
compaction tumbstone (deleted marker)

Bloom filter garanteed true negatives

TimeWindowCompaction
SizeTieredCompaction
LeveledCompaction

materialed view
keyspace: bag of tables with replication factor, similar to schema
class: SimpleStategy or NetworkStrategy

Cassandra: up to 10Tb per node, HDFS has better density

rack - aware, region - aware
partition key is subset of Primary Key
Columns and tables support an optional expiration period called TTL (time-to-live); TTL is not supported on counter columns. Define the TTL value in seconds. Data expires once it exceeds the TTL period and is then marked with a tombstone. Expired data continues to be available for read requests during the grace period, see gc_grace_seconds. Normal compaction and repair processes automatically remove the tombstone data.


mirmur3_hash(primary_key)  -> token   (range of keys)
Keyspace attributes:
- replication factor
- replica placement strategy
- column family for keyspace

cqlsh> describe keyspaces
select columnfamily_name from system.schema_columnfamilies where keyspace_name = 'test';
or
cqlsh> use products;
cqlsh:products> describe tables;    or DESCRIBE COLUMNFAMILIES;

if nodes are non in sync then node with latest data wins
http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html
Consistecy levels (per statement read/write) for reads and writes:
ALL - all replicas ack
QUORUME > 51%
LOCAL_QUORUM in local DC
ONE
TWO


write:  commit log and to memtable -> SSTable
cassandra.yml    fsynced every 10s


http://abiasforaction.net/cassandra-query-language-cql-tutorial/

https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/

https://www.instaclustr.com/resource/cassandra-nosql-data-model-design/
https://www.instaclustr.com/cassandra-nosql-data-model-design-2/
https://www.instaclustr.com/apache-cassandra-3-x-and-materialized-views/

https://tekslate.com/cassandra-interview-questions-and-answers

https://opencredo.com/how-not-to-use-cassandra-like-an-rdbms-and-what-will-happen-if-you-do/
https://opencredo.com/cassandra-data-modelling-patterns/
https://opencredo.com/training/cassandra-fundamentals-data-modelling/
https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04
https://vitobotta.com/2017/04/02/getting-started-apache-cassandra/

http://cassandra.alteroot.org/
http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html
http://lucasmanual.com/blog/quick-intro-to-cassandra-vs-mongodb-with-python/
http://blogs.shephertz.com/2015/04/22/why-cassandra-excellent-choice-for-realtime-analytics-workload/
https://opencredo.com/spark-testing/

https://www.youtube.com/watch?v=Mvsy2DZhKSw
https://www.youtube.com/watch?v=HuDJBTPdaOA
https://www.reddit.com/r/cassandra/comments/6l01iu/wide_row_append_only_timeseries_data_no_ttl_what/





https://www.instaclustr.com/resource/apache-spark-apache-cassandra-powering-intelligent-applications/
https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/
log-structured merge trees (LSMT)
LSM trees  just dumping stuff at the end of an array. However this means you have to scan that array to do lookups (as opposed to something easier like binary search). Then as your array gets big, you merge and flush it down to a lower "layer" of the lsm tree which is slightly bigger and sorted. And when that one fills, you flush further. And these merge-flushes are nice big sequential writes so that's nice too.
http://neovintage.org/2016/04/07/data-modeling-in-cassandra-from-a-postgres-perspective/

https://www.goetas.com/blog/how-i-approached-software-development-and-why-i-prefer-postgresql-to-mysql/

<h2>TimeSeries</h2>
https://blog.outlyer.com/top10-open-source-time-series-databases
https://news.ycombinator.com/item?id=15649405

<h2>Postgres</h2>
https://blog.timescale.com/scaling-partitioning-data-postgresql-10-explained-cd48a712a9a1
http://www.craigkerstiens.com/categories/postgresql/
https://habrahabr.ru/post/340460/
https://10clouds.com/blog/postgresql-10/
https://news.ycombinator.com/item?id=15634953
http://vvvvalvalval.github.io/posts/using-postgresql-temporary-views-for-expressing-business-logic.html

<h2>Analytics</h2>
https://www.analyticsvidhya.com/blog/2014/09/commonly-asked-puzzles-analytics-interviews/
https://www.reddit.com/r/predictiveanalytics/
https://dzone.com/big-data-analytics-tutorials-tools-news
https://dzone.com/articles/streamline-the-machine-learning-process-using-apac
https://medium.com/rv-data/mleap-providing-near-real-time-data-science-with-apache-spark-c34e7df093ca
http://quasar-analytics.org/   Scala engine for analytics
</pre>